{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc6c12e",
   "metadata": {},
   "source": [
    "## 텍스트 분류 - 뉴스\n",
    "---\n",
    "- scikit-learn의 dataset인 20대뉴스 데이터 분류\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd906269",
   "metadata": {},
   "source": [
    "## [1] 데이터 준비\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43644368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751aca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsData=fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54549acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, str)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData20 = newsData[\"data\"]\n",
    "type(newsData20), type(newsData20[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1407596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsData20[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1459e",
   "metadata": {},
   "source": [
    "## [2] 데이터 전처리\n",
    "---\n",
    "- 데이터 구조 확인\n",
    "    * 목적에 맞는 데이터 여부 확인\n",
    "- 단어사전 생성\n",
    "- 텍스트 데이터 > 정수 수치화\n",
    "- 이진 수치화 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372defc",
   "metadata": {},
   "source": [
    "#### [2-1] 단어사전 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b6d8f",
   "metadata": {},
   "source": [
    "#### [2-1-1] 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f953aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba036f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 코프스 다운로드 받기\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097db6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 블용어(stopword) 분석에 영향을 미치지 않는 단어들  => 예) i my to and ... \n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45d6d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'stopwords : {stopwords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be0f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c951383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제(Cleaning) - 불용어 제거 \n",
    "def cleaningData(removeData):\n",
    "    for idx in range(len(newsData20)):\n",
    "        news=text_to_word_sequence(newsData20[idx])\n",
    "        _clear=[]\n",
    "        for n in news:\n",
    "            if n not in removeData:\n",
    "                _clear.append(n)\n",
    "        newsData20[idx]=' '.join(_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "792ff536",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaningData(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e4a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sure bashers pens fans pretty confused lack kind posts recent pens massacre devils actually bit puzzled bit relieved however going put end non pittsburghers' relief bit praise pens man killing devils worse thought jagr showed much better regular season stats also lot fo fun watch playoffs bowman let jagr lot fun next couple games since pens going beat pulp jersey anyway disappointed see islanders lose final regular season game pens rule\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData20[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa1dac",
   "metadata": {},
   "source": [
    "##### [2-2-2] 단어사전 생성 및 사용할 단어사전 수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ad116c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordVoca(numWord=0):\n",
    "    if numWord>0:\n",
    "        myToken=Tokenizer(num_words=numWord)\n",
    "    else:\n",
    "        myToken=Tokenizer()\n",
    "    \n",
    "    # 단어사전(voca) 생성\n",
    "    myToken.fit_on_texts(newsData20)\n",
    "    \n",
    "    return myToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8da8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index : 139318개\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 토큰화 진행\n",
    "myToken=makeWordVoca()\n",
    "print(f'word_index : {len(myToken.word_index)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a7312b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전의 사용빈도 높은것을 기준으로 단어사전 크기 제한\n",
    "def getNumWord(limit_num):\n",
    "    # 빈도수가 limit_num개 인것 체크 후 제거\n",
    "    low_freq_cnt=0\n",
    "    for k, v in myToken.word_counts.items():\n",
    "        if v == limit_num: low_freq_cnt += 1\n",
    "\n",
    "    return len(myToken.word_index) - low_freq_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249ecd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_word : 121055개\n"
     ]
    }
   ],
   "source": [
    "num_word = getNumWord(2)\n",
    "print(f'num_word : {num_word}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab800da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index : 139318개\n"
     ]
    }
   ],
   "source": [
    "myToken=makeWordVoca(num_word)\n",
    "print(f'word_index : {len(myToken.word_index)}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851c244",
   "metadata": {},
   "source": [
    "##### [2-2-2-3] 텍스트 => 정수 수치화 (  생성한 단어사전 기반 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45fe7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 => 수치화 (단어 사전 사용해서)\n",
    "seq_news=myToken.texts_to_sequences(newsData20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ae9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_news : 18846\n"
     ]
    }
   ],
   "source": [
    "print(f'seq_news : {len(seq_news)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8626fc34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
