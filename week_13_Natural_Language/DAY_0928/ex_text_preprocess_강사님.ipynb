{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1cbb8",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "----\n",
    "- 패키지 설치\n",
    "    * NLTK : pip install nltk\n",
    "    * KoNLPy : pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8cb03a2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지 설치\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e619b35b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80be6",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    "- 문장/문서를 의미를 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함\n",
    "- 종류\n",
    "    * 문장 토큰화\n",
    "    * 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcafe1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1a6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5236e165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Corpus 말뭉치 데이터셋 다운로드 받기\n",
    "nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cf749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1=\"hen tokenizing a Unicode string.\\\n",
    "           NLTK tokenizers can produce token-spans.\\\n",
    "           hen tokenizing a Unicode string.\"\n",
    "raw_text2=\"This particular tokenizer requires the Punkt sentence tokenization.\\\n",
    "           which splits text on whitespace and punctuation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5650837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1=word_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc0cc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen', 'tokenizing', 'a', 'Unicode', 'string', '.', 'NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.', 'hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e54dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위 토큰화\n",
    "raw_text= raw_text1+raw_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec60f3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hen tokenizing a Unicode string.           NLTK tokenizers can produce token-spans.           hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.           which splits text on whitespace and punctuation.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1fc428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=sent_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae8dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen tokenizing a Unicode string.', 'NLTK tokenizers can produce token-spans.', 'hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.', 'which splits text on whitespace and punctuation.'] 4\n"
     ]
    }
   ],
   "source": [
    "print(result, len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "839a695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hen tokenizing a Unicode string.',\n",
       " 'NLTK tokenizers can produce token-spans.',\n",
       " 'hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.',\n",
       " 'which splits text on whitespace and punctuation.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb3aea",
   "metadata": {},
   "source": [
    "### 여러 문장에 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d84573d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => ['hen tokenizing a Unicode string.']\n",
      "ele => hen tokenizing a Unicode string.\n",
      "wordResult => ['hen', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "sent => ['NLTK tokenizers can produce token-spans.']\n",
      "ele => NLTK tokenizers can produce token-spans.\n",
      "wordResult => ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', '.']\n",
      "sent => ['hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.']\n",
      "ele => hen tokenizing a Unicode string.This particular tokenizer requires the Punkt sentence tokenization.\n",
      "wordResult => ['hen', 'tokenizing', 'a', 'Unicode', 'string.This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', '.']\n",
      "sent => ['which splits text on whitespace and punctuation.']\n",
      "ele => which splits text on whitespace and punctuation.\n",
      "wordResult => ['which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "for sent in result:\n",
    "    total_token=set()\n",
    "    #문장 추출\n",
    "    sentResult=sent_tokenize(sent)\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f'sent => {sentResult}')\n",
    "    \n",
    "    for ele in sentResult:\n",
    "        print(f'ele => {ele}')\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(f'wordResult => {wordResult}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949d79f",
   "metadata": {},
   "source": [
    "#### 한글 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2d0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 행태소 분리 객체\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b340d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '월요일', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리\n",
    "result=okt.morphs(\"오늘은 월요일입니다.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82e964dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행태소 분리 후 태깅(Tagging) => 품사\n",
    "result2=okt.pos(\"오늘은 월요일입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc478f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b7f682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=okt.pos(\"오늘은 월요일입니다.\", stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25d1c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d57f8b",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화 \n",
    "    * 대문자 또는 소문자로 통일\n",
    "    * 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c894dd7",
   "metadata": {},
   "source": [
    "### [2-1] 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "140b15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "500b553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12703e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f7c7b",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c79d125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d224ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "lstem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7d520d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work', 'work')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('working'), lstem.stem('worked'), lstem.stem('worken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c41f4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bc53069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c721beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11bfe0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dce8666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('working', 'v'), wlemma.lemmatize('worked', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb55370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('amusing', 'v'), wlemma.lemmatize('amused', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47d7b3",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    "- 텍스트 => 수치화\n",
    "- 희소벡터(OHE) : BOW 방식 -->  Count기반, TF-IDF 기반\n",
    "- 밀집벡터 : Embedding 방식 , Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e46ed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d483577",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7b2129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24560c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c31e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ohe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b64827e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 17)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t2\n",
      "  (0, 22)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 24)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ret), ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4159707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ret.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1139f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "[[0 1 2 1 0 0 1 0 0 0 0 1 0 2 0 0 0 1 0 0 1 2 2 0 0]\n",
      " [1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(ret.shape, ret, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4eb714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF 기반\n",
    "tfIdf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a223096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfIdf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0269f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebc9f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus= tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01331cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.21320072 0.42640143 0.21320072 0.         0.\n",
      "  0.21320072 0.         0.         0.         0.         0.21320072\n",
      "  0.         0.42640143 0.         0.         0.         0.21320072\n",
      "  0.         0.         0.21320072 0.42640143 0.42640143 0.\n",
      "  0.        ]\n",
      " [0.25819889 0.         0.         0.         0.25819889 0.25819889\n",
      "  0.         0.25819889 0.25819889 0.25819889 0.25819889 0.\n",
      "  0.25819889 0.         0.25819889 0.25819889 0.25819889 0.\n",
      "  0.25819889 0.25819889 0.         0.         0.         0.25819889\n",
      "  0.25819889]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766af7c",
   "metadata": {},
   "source": [
    "## 실습 --------------------------------------------------\n",
    "---\n",
    "- 단어 단위 토큰화\n",
    "- 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd3320a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#볼용어 추출\n",
    "from nltk import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f146b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e870c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7affc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens=word_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08b4440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, list)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordTokens), type(wordTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b94ae93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens2 : 85\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "wordTokens2=[]\n",
    "for word in wordTokens:\n",
    "    if word not in en_stopwords:\n",
    "        wordTokens2.append(word)\n",
    "\n",
    "print(f'wordTokens2 : {len(wordTokens2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "872f5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTokens3 : 85\n"
     ]
    }
   ],
   "source": [
    "wordTokens3=[ word for word in wordTokens if word not in en_stopwords ]\n",
    "\n",
    "print(f'wordTokens3 : {len(wordTokens2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56642e0",
   "metadata": {},
   "source": [
    "## Tokenizer 객체 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "730f9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "054da984",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83ecd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "498827a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25395069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e96014",
   "metadata": {},
   "source": [
    "### Tokenizer 객체 --------------------------------------------------------------------\n",
    "- 제공한 문서/문장에 대한 단어사전(voca)\n",
    "- 단어사전(voca)에 존재하지 않는 단어 => Out Of Voca : oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "470f8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!',\n",
    "  'Do you think my dog is amazing?'\n",
    "]\n",
    "# {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, '\n",
    "#  do': 7, 'think': 8, 'is': 9, 'amazing': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfb06559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='oov', num_words=5)\n",
    "\n",
    "# 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c62af847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oov': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# 단어 인덱스  : 단어 인덱스\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56ebed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('i', 2), ('love', 3), ('my', 4), ('dog', 3), ('cat', 1), ('you', 2), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 단어 출력 갯수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1df3927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 2, 4, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화 \n",
    "print(tokenizer.texts_to_sequences(['We think my dog is amazing?']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52759a9",
   "metadata": {},
   "source": [
    "### 패딩(Padding)\n",
    "---\n",
    "- 길이가 모두 다른 문장들을 동일 길이로 맞추기 위한 과정\n",
    "- 길이 기준 설정\n",
    "- 긴 경우 => 앞/뒤 중 선택\n",
    "- 짧은 경우 =>  앞/뒤 중 선택\n",
    "- 값 => 패딩에 들어갈 값    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0904cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c240ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5033423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 2, 4], [1, 3, 2, 1], [1, 3, 2, 4], [1, 1, 1, 2, 4, 1, 1]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "422c005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 3, 2, 4],\n",
       "       [0, 0, 0, 1, 3, 2, 1],\n",
       "       [0, 0, 0, 1, 3, 2, 4],\n",
       "       [1, 1, 1, 2, 4, 1, 1]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding=pad_sequences(result)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8d7f1",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding 변환\n",
    "---\n",
    "- sklearn OneHotEncoder객체 생성\n",
    "- kears 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88497b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "212d0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_voca : 4\n",
      "[[1, 3, 2, 4], [1, 3, 2, 1], [1, 3, 2, 4], [1, 1, 1, 2, 4, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화 \n",
    "seq_voca=tokenizer.texts_to_sequences(sentences)\n",
    "print(f'seq_voca : {len(seq_voca)}')\n",
    "print(seq_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "877fef1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(seq_voca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b10e459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6961943c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 2, 4], [1, 3, 2, 1], [1, 3, 2, 4], [1, 1, 1, 2, 4, 1, 1]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be0240bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc57cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 2, 4], [1, 3, 2, 1], [1, 3, 2, 4], [1, 1, 1, 2, 4, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1345bc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 3, 2, 4],\n",
       "       [0, 0, 0, 1, 3, 2, 1],\n",
       "       [0, 0, 0, 1, 3, 2, 4],\n",
       "       [1, 1, 1, 2, 4, 1, 1]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c11b050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 2, 4, 0, 0, 0],\n",
       "       [1, 3, 2, 1, 0, 0, 0],\n",
       "       [1, 3, 2, 4, 0, 0, 0],\n",
       "       [1, 1, 1, 2, 4, 1, 1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d04a7645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 2, 4, 0],\n",
       "       [1, 3, 2, 1, 0],\n",
       "       [1, 3, 2, 4, 0],\n",
       "       [1, 2, 4, 1, 1]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24fd49",
   "metadata": {},
   "source": [
    "## FILE 읽고 벡터화\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89180d",
   "metadata": {},
   "source": [
    "#### [1] 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0da092ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE='../data/example.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49893d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE, mode='r') as f:\n",
    "    fileData=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a89c38f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(fileData), type(fileData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9054b440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy\\n\\n\\nHenry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution. The school is sponsored by the Ford Motor Company, Wayne County Regional Educational Service Agency and The Henry Ford Museum and admits high school students. It is located in Dearborn, Michigan on the campus of the Henry Ford museum. Enrollment is taken from a lottery in the area and totaled 467 in 2010.[1]\\nFreshman meet inside the main museum building in glass walled classrooms, while older students use a converted carousel building and Pullman cars on a siding of the Greenfield Village railroad. Classes are expected to include use of the museum artifacts, a tradition of the original Village Schools. When the Museum was established in 1929, it included a school which served grades kindergarten to college/trade school ages. The last part of the original school closed in 1969.\\nThe Henry Ford Learning Institute is using the Henry Ford Academy model for further charter schools including the Power House High in Chicago and Alameda School for Art + Design in San Antonio.\\nThe building received the international annual design award of the Council of Educational Facilities Planners International for 2001, the James D. MacConnell Award for outstanding new educational facilities. Notable attendees include Chris Stroud and Isaac Sudut.\\nSee also[edit]'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "59e260fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 => 문자열 리스트\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "data_list=sent_tokenize(fileData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "971d08d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list => 12\n"
     ]
    }
   ],
   "source": [
    "print(f'data_list => {len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ee2b8642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy\\n\\n\\nHenry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution.',\n",
       " 'The school is sponsored by the Ford Motor Company, Wayne County Regional Educational Service Agency and The Henry Ford Museum and admits high school students.',\n",
       " 'It is located in Dearborn, Michigan on the campus of the Henry Ford museum.',\n",
       " 'Enrollment is taken from a lottery in the area and totaled 467 in 2010.',\n",
       " '[1]\\nFreshman meet inside the main museum building in glass walled classrooms, while older students use a converted carousel building and Pullman cars on a siding of the Greenfield Village railroad.',\n",
       " 'Classes are expected to include use of the museum artifacts, a tradition of the original Village Schools.',\n",
       " 'When the Museum was established in 1929, it included a school which served grades kindergarten to college/trade school ages.',\n",
       " 'The last part of the original school closed in 1969.',\n",
       " 'The Henry Ford Learning Institute is using the Henry Ford Academy model for further charter schools including the Power House High in Chicago and Alameda School for Art + Design in San Antonio.',\n",
       " 'The building received the international annual design award of the Council of Educational Facilities Planners International for 2001, the James D. MacConnell Award for outstanding new educational facilities.',\n",
       " 'Notable attendees include Chris Stroud and Isaac Sudut.',\n",
       " 'See also[edit]']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e566bc1",
   "metadata": {},
   "source": [
    "##### [2] 토큰화 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20eb247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToken=Tokenizer()\n",
    "# raw_data용 단어사전 생성\n",
    "fileToken.fit_on_texts(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "36e9a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index : 137개\n",
      "{'the': 1, 'in': 2, 'ford': 3, 'of': 4, 'henry': 5, 'school': 6, 'a': 7, 'and': 8, 'museum': 9, 'for': 10, 'is': 11, 'building': 12, 'academy': 13, 'to': 14, 'educational': 15, 'main': 16, 'classrooms': 17, 'charter': 18, 'by': 19, 'high': 20, 'students': 21, 'it': 22, 'on': 23, 'use': 24, 'village': 25, 'include': 26, 'original': 27, 'schools': 28, 'design': 29, 'international': 30, 'award': 31, 'facilities': 32, 'houses': 33, 'some': 34, 'first': 35, 'united': 36, 'states': 37, 'be': 38, 'developed': 39, 'jointly': 40, 'global': 41, 'corporation': 42, 'public': 43, 'education': 44, 'major': 45, 'nonprofit': 46, 'cultural': 47, 'institution': 48, 'sponsored': 49, 'motor': 50, 'company': 51, 'wayne': 52, 'county': 53, 'regional': 54, 'service': 55, 'agency': 56, 'admits': 57, 'located': 58, 'dearborn': 59, 'michigan': 60, 'campus': 61, 'enrollment': 62, 'taken': 63, 'from': 64, 'lottery': 65, 'area': 66, 'totaled': 67, '467': 68, '2010': 69, '1': 70, 'freshman': 71, 'meet': 72, 'inside': 73, 'glass': 74, 'walled': 75, 'while': 76, 'older': 77, 'converted': 78, 'carousel': 79, 'pullman': 80, 'cars': 81, 'siding': 82, 'greenfield': 83, 'railroad': 84, 'classes': 85, 'are': 86, 'expected': 87, 'artifacts': 88, 'tradition': 89, 'when': 90, 'was': 91, 'established': 92, '1929': 93, 'included': 94, 'which': 95, 'served': 96, 'grades': 97, 'kindergarten': 98, 'college': 99, 'trade': 100, 'ages': 101, 'last': 102, 'part': 103, 'closed': 104, '1969': 105, 'learning': 106, 'institute': 107, 'using': 108, 'model': 109, 'further': 110, 'including': 111, 'power': 112, 'house': 113, 'chicago': 114, 'alameda': 115, 'art': 116, 'san': 117, 'antonio': 118, 'received': 119, 'annual': 120, 'council': 121, 'planners': 122, '2001': 123, 'james': 124, 'd': 125, 'macconnell': 126, 'outstanding': 127, 'new': 128, 'notable': 129, 'attendees': 130, 'chris': 131, 'stroud': 132, 'isaac': 133, 'sudut': 134, 'see': 135, 'also': 136, 'edit': 137} \n"
     ]
    }
   ],
   "source": [
    "print(f'word_index : { len( fileToken.word_index)}개\\n{ fileToken.word_index } ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493b6f5",
   "metadata": {},
   "source": [
    "##### [3] 문장 수치화 & 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8be47449",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqData=fileToken.texts_to_sequences(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d08728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 5, 3, 9, 12, 33, 34, 4, 1, 17, 10, 1, 5, 3, 13, 5, 3, 13, 11, 1, 35, 18, 6, 2, 1, 36, 37, 14, 38, 39, 40, 19, 7, 41, 42, 43, 44, 8, 7, 45, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "print(seqData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a5be5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy\n",
      "\n",
      "\n",
      "Henry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution.\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
